# **âœˆï¸ Proyecto ETL Enterprise-Grade de Datos de Vuelos**

**Proyecto\_Databricks \- SmartData**

**DescripciÃ³n:** Pipeline ETL *enterprise-grade* que transforma datos crudos de vuelos del 2015, implementando la **Arquitectura Medallion** (Bronze-Silver-Gold) en Azure Databricks con **Unity Catalog** para la gobernanza y **CI/CD completo** con **GitHub Actions** para garantizar la automatizaciÃ³n del despliegue y la consistencia ACID con **Delta Lake**.

## **âœ¨ CaracterÃ­sticas Principales**

* ğŸ”„ **ETL Automatizado** \- Pipeline completo con despliegue automÃ¡tico vÃ­a GitHub Actions.  
* ğŸ—ï¸ **Arquitectura Medallion** \- SeparaciÃ³n clara de capas, incluyendo la correcciÃ³n explÃ­cita de tipos en las capas Silver y Golden.  
* ğŸ”’ **MitigaciÃ³n de Fallos** \- Tarea de **FALLBACK\_REVOKE** que borra los cambios realizados en el job en las diferentes capas antes del error.  
* ğŸš€ **CI/CD Integrado** \- Despliegue automÃ¡tico de *notebooks* y *workflows* en cada *push* a la rama principal.  
* âš¡ **Delta Lake** \- Garantiza transacciones ACID, *schema evolution* y *time travel* capabilities.  
* ğŸ›ï¸ **Unity Catalog** \- Gobernanza centralizada y control de acceso granular sobre catÃ¡logos, esquemas y tablas.

## **ğŸ›ï¸ Arquitectura**

El flujo de datos sigue el patrÃ³n Medallion, garantizando la calidad progresiva de los datos.

### **Flujo de Datos**

ğŸ“„ Origen CSV (Raw Data)  
â†“  
ğŸ¥‰ Bronze Layer (Ingesta de vuelos, aeropuertos y aerolÃ­neas)  
â†“  
ğŸ¥ˆ Silver Layer (Limpieza de datos y Tablas de DimensiÃ³n)  
â†“  
ğŸ¥‡ Golden Layer (Agregaciones de Vuelos y KPIs)  
â†“  
ğŸ“Š Consumo BI (Power BI)

### **ğŸ“¦ Capas del Pipeline**

#### **ğŸ¥‰ Bronze Layer**

PropÃ³sito: Zona de aterrizaje y fuente de verdad histÃ³rica.  
Tablas: flights, airlines, airports.  
CaracterÃ­sticas:

* âœ… Datos tal como vienen de origen.  
* âœ… MÃ­nima manipulaciÃ³n.  
* âœ… PreservaciÃ³n de la historia de los datos crudos.

#### **ğŸ¥ˆ Silver Layer**

PropÃ³sito: CreaciÃ³n de tablas limpias y dimensionales.  
Tablas: Tablas de hechos con datos limpios y dimensiones.  
CaracterÃ­sticas:

* âœ… Datos validados y estandarizados.  
* âœ… EliminaciÃ³n/gestiÃ³n de nulos.  
* âœ… PreparaciÃ³n para el modelo dimensional.

#### **ğŸ¥‡ Golden Layer**

PropÃ³sito: Capa de reportes y KPIs Analytics-ready.  
Tablas:

* RPT\_RESUMEN\_VUELOS\_DIARIO: Resumen de la actividad diaria (distancia total, retrasos promedio, cancelaciones, etc.).  
* TM\_TIEMPO: DimensiÃ³n de tiempo final.  
  CaracterÃ­sticas:  
* âœ… Pre-agregados y optimizados para consultas de BI.  
* âœ… InclusiÃ³n de la correcciÃ³n explÃ­cita de tipos de datos.

## **ğŸ“ Estructura del Proyecto**

Proyecto\_Databricks/  
â”‚  
â”œâ”€â”€ ğŸ“‚ .github/  
â”‚   â””â”€â”€ ğŸ“‚ workflows/  
â”‚       â””â”€â”€ ğŸ“„ script\_Prod.yml            \# Pipeline CI/CD: Despliega y orquesta el Job WF\_ADB  
â”œâ”€â”€ ğŸ“‚ certificaciones/  
â”‚   â””â”€â”€ ğŸ“„ Certificaciones\_Databricks.txt  \# DocumentaciÃ³n o evidencia de certificaciones.  
â”œâ”€â”€ ğŸ“‚ proceso/  
â”‚   â”œâ”€â”€ ğŸ 1\_raw\_to\_bronze.py            \# Tarea 2: Ingesta de datos crudos (Bronze Layer).  
â”‚   â”œâ”€â”€ ğŸ 2\_bronze\_to\_silver.py         \# Tarea 3: Limpieza y enriquecimiento (Silver Layer).  
â”‚   â””â”€â”€ ğŸ 3\_silver\_to\_golden.py         \# Tarea 4: AgregaciÃ³n de KPIs y Reportes (Gold Layer).  
â”œâ”€â”€ ğŸ“‚ reversion/  
â”‚   â””â”€â”€ ğŸ Revoke.py                     \# Tarea 6: Revoca permisos (LÃ³gica de Fallback/MitigaciÃ³n de Fallos).  
â”œâ”€â”€ ğŸ“‚ scripts/  
â”‚   â””â”€â”€ ğŸ Preparacion\_Catalogo.py       \# Tarea 1: Crea CatÃ¡logo, Esquemas y configura Unity Catalog.  
â”œâ”€â”€ ğŸ“‚ seguridad/  
â”‚   â””â”€â”€ ğŸ Grants.py                     \# Tarea 5: Otorga permisos SELECT a grupos de consumo (Golden Layer).  
â”œâ”€â”€ ğŸ“„ Job\_Flights\_Completo.png          \# Diagrama del Workflow de Databricks Jobs (WF\_ADB).  
â””â”€â”€ ğŸ“„ README.md

## **ğŸ› ï¸ TecnologÃ­as**

| TecnologÃ­a | PropÃ³sito |
| :---- | :---- |
| **Azure Databricks** | Plataforma unificada de datos y motor de procesamiento Spark. |
| **Unity Catalog** | Capa de gobernanza de datos, control de acceso y gestiÃ³n de metadatos. |
| **Delta Lake** | Storage layer que garantiza transacciones ACID para las tablas del pipeline. |
| **GitHub Actions** | AutomatizaciÃ³n completa del flujo CI/CD. |
| **Python / PySpark** | Lenguajes primarios para la transformaciÃ³n y orquestaciÃ³n de datos. |
| **Databricks Jobs API** | CreaciÃ³n y ejecuciÃ³n programÃ¡tica del *workflow* en ProducciÃ³n. |

## **âš™ï¸ Requisitos Previos**

* â˜ï¸ Cuenta de Azure con acceso a Databricks.  
* ğŸ’» Workspace de Databricks configurado (Desarrollo y ProducciÃ³n).  
* ğŸ–¥ï¸ Cluster activo en el entorno de ProducciÃ³n.  
* ğŸ™ Cuenta de GitHub con permisos de administrador en el repositorio.

## **ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n**

El despliegue se basa en la configuraciÃ³n de **Secrets** de GitHub.

### **Configurar GitHub Secrets**

En tu repositorio: Settings â†’ Secrets and variables â†’ Actions.

| Secret Name | PropÃ³sito |
| :---- | :---- |
| DATABRICKS\_DEST\_HOST | URL del Workspace de ProducciÃ³n |
| DATABRICKS\_DEST\_TOKEN | Token de acceso personal (PAT) para el Workspace de ProducciÃ³n con permisos de Job, Cluster y Workspace. |
| CLUSTER\_ID | ID del Cluster en ProducciÃ³n |

## **ğŸ’» Uso (Despliegue y EjecuciÃ³n)**

### **ğŸ”„ Despliegue AutomÃ¡tico (Recomendado)**

El *workflow* **script\_Prod.yml** se activa con cualquier *push* a la rama main.

**GitHub Actions ejecutarÃ¡:**

* ğŸ“¤ Despliegue de todos los *notebooks* a /Proyecto\_Flights.  
* ğŸ”§ CreaciÃ³n/ActualizaciÃ³n del *workflow* WF\_ADB.  
* â–¶ï¸ EjecuciÃ³n inmediata de todo el pipeline: `PreparaciÃ³n` **â†’** `Bronze` **â†’** `Silver` **â†’** `Gold`.  
* Monitoreo del Job hasta su finalizaciÃ³n.

### **ğŸ”„ OrquestaciÃ³n del Workflow Databricks (WF\_ADB)**

Este Job estÃ¡ diseÃ±ado para ser tolerante a fallos en la capa de consumo:

| Tarea | CondiciÃ³n de EjecuciÃ³n |
| :---- | :---- |
| **Tareas 1 a 5 (ETL \+ Grants)** | Se ejecutan en secuencia normal (Dependencias). |
| **Tarea 6 (Revoke)** | **All Done / At Least One Failed**. |

**FunciÃ³n de la Tarea 6 (`Revoke`):** Esta tarea de contingencia se ejecuta siempre que el Job haya finalizado (Ã©xito o fallo). Su funciÃ³n es realizar una reversiÃ³n completa (Rollback):

* Elimina todas las tablas creadas y llenadas por las tareas 1 a 4\.  
* Elimina las rutas de almacenamiento (archivos) asociadas a estas tablas en el Data Lake.

Esto asegura que no quede cÃ³digo ni datos parciales o corruptos en el entorno de ProducciÃ³n.

## **ğŸ‘¤ Autor**

Jose Carlos Gonzales Espinoza

Data Engineering | Azure Databricks | Delta Lake | CI/CD